{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing image 1/93\n",
      "Processing image 2/93\n",
      "Processing image 3/93\n",
      "Processing image 4/93\n",
      "Processing image 5/93\n",
      "Processing image 6/93\n",
      "Processing image 7/93\n",
      "Processing image 8/93\n",
      "Processing image 9/93\n",
      "Processing image 10/93\n",
      "Processing image 11/93\n",
      "Processing image 12/93\n",
      "Processing image 13/93\n",
      "Processing image 14/93\n",
      "Processing image 15/93\n",
      "Processing image 16/93\n",
      "Processing image 17/93\n",
      "Processing image 18/93\n",
      "Processing image 19/93\n",
      "Processing image 20/93\n",
      "Processing image 21/93\n",
      "Processing image 22/93\n",
      "Processing image 23/93\n",
      "Processing image 24/93\n",
      "Processing image 25/93\n",
      "Processing image 26/93\n",
      "Processing image 27/93\n",
      "Processing image 28/93\n",
      "Processing image 29/93\n",
      "Processing image 30/93\n",
      "Processing image 31/93\n",
      "Processing image 32/93\n",
      "Processing image 33/93\n",
      "Processing image 34/93\n",
      "Processing image 35/93\n",
      "Processing image 36/93\n",
      "Processing image 37/93\n",
      "Processing image 38/93\n",
      "Processing image 39/93\n",
      "Processing image 40/93\n",
      "Processing image 41/93\n",
      "Processing image 42/93\n",
      "Processing image 43/93\n",
      "Processing image 44/93\n",
      "Processing image 45/93\n",
      "Processing image 46/93\n",
      "Processing image 47/93\n",
      "Processing image 48/93\n",
      "Processing image 49/93\n",
      "Processing image 50/93\n",
      "Processing image 51/93\n",
      "Processing image 52/93\n",
      "Processing image 53/93\n",
      "Processing image 54/93\n",
      "Processing image 55/93\n",
      "Processing image 56/93\n",
      "Processing image 57/93\n",
      "Processing image 58/93\n",
      "Processing image 59/93\n",
      "Processing image 60/93\n",
      "Processing image 61/93\n",
      "Processing image 62/93\n",
      "Processing image 63/93\n",
      "Processing image 64/93\n",
      "Processing image 65/93\n",
      "Processing image 66/93\n",
      "Processing image 67/93\n",
      "Processing image 68/93\n",
      "Processing image 69/93\n",
      "Processing image 70/93\n",
      "Processing image 71/93\n",
      "Processing image 72/93\n",
      "Processing image 73/93\n",
      "Processing image 74/93\n",
      "Processing image 75/93\n",
      "Processing image 76/93\n",
      "Processing image 77/93\n",
      "Processing image 78/93\n",
      "Processing image 79/93\n",
      "Processing image 80/93\n",
      "Processing image 81/93\n",
      "Processing image 82/93\n",
      "Processing image 83/93\n",
      "Processing image 84/93\n",
      "Processing image 85/93\n",
      "Processing image 86/93\n",
      "Processing image 87/93\n",
      "Processing image 88/93\n",
      "Processing image 89/93\n",
      "Processing image 90/93\n",
      "Processing image 91/93\n",
      "Processing image 92/93\n",
      "Processing image 93/93\n",
      "Embedding:50 \n",
      "Process Completed\n"
     ]
    }
   ],
   "source": [
    "from imutils import paths\n",
    "import numpy as np\n",
    "import imutils\n",
    "import pickle\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "dataset = \"dataset\"\n",
    "\n",
    "embeddingFile = \"output/embeddings.pickle\" #initial name for embedding file\n",
    "embeddingModel = \"openface_nn4.small2.v1.t7\" #initializing model for embedding Pytorch\n",
    "\n",
    "#initialization of caffe model for face detection\n",
    "prototxt = \"model/deploy.prototxt\"\n",
    "model =  \"model/res10_300x300_ssd_iter_140000.caffemodel\"\n",
    "\n",
    "#loading caffe model for face detection\n",
    "#detecting face from Image via Caffe deep learning\n",
    "detector = cv2.dnn.readNetFromCaffe(prototxt, model)\n",
    "\n",
    "#loading pytorch model file for extract facial embeddings\n",
    "#extracting facial embeddings via deep learning feature extraction\n",
    "embedder = cv2.dnn.readNetFromTorch(embeddingModel)\n",
    "\n",
    "#gettiing image paths\n",
    "imagePaths = list(paths.list_images(dataset))\n",
    "\n",
    "#initialization\n",
    "knownEmbeddings = []\n",
    "knownNames = []\n",
    "total = 0\n",
    "conf = 0.5\n",
    "\n",
    "#we start to read images one by one to apply face detection and embedding\n",
    "for (i, imagePath) in enumerate(imagePaths):\n",
    "    print(\"Processing image {}/{}\".format(i + 1,len(imagePaths)))\n",
    "    name = imagePath.split(os.path.sep)[-2]\n",
    "    image = cv2.imread(imagePath)\n",
    "    image = imutils.resize(image, width=600)\n",
    "    (h, w) = image.shape[:2]\n",
    "    #converting image to blob for dnn face detection\n",
    "    imageBlob = cv2.dnn.blobFromImage(\n",
    "        cv2.resize(image, (300, 300)), 1.0, (300, 300),(104.0, 177.0, 123.0), swapRB=False, crop=False)\n",
    "\n",
    "    #setting input blob image\n",
    "    detector.setInput(imageBlob)\n",
    "    #prediction the face\n",
    "    detections = detector.forward()\n",
    "\n",
    "    if len(detections) > 0:\n",
    "        i = np.argmax(detections[0, 0, :, 2])\n",
    "        confidence = detections[0, 0, i, 2]\n",
    "\n",
    "        if confidence > conf:\n",
    "            #ROI range of interest\n",
    "            box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
    "            (startX, startY, endX, endY) = box.astype(\"int\")\n",
    "            face = image[startY:endY, startX:endX]\n",
    "            (fH, fW) = face.shape[:2]\n",
    "            if fW < 20 or fH < 20:\n",
    "                continue\n",
    "            #image to blob for face\n",
    "            faceBlob = cv2.dnn.blobFromImage(face, 1.0 / 255, (96, 96), (0, 0, 0), swapRB=True, crop=False)\n",
    "            #facial features embedder input image face blob\n",
    "            embedder.setInput(faceBlob)\n",
    "            vec = embedder.forward()\n",
    "            knownNames.append(name)\n",
    "            knownEmbeddings.append(vec.flatten())\n",
    "            total += 1\n",
    "\n",
    "print(\"Embedding:{0} \".format(total))\n",
    "data = {\"embeddings\": knownEmbeddings, \"names\": knownNames}\n",
    "f = open(embeddingFile, \"wb\")\n",
    "f.write(pickle.dumps(data))\n",
    "f.close()\n",
    "print(\"Process Completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
